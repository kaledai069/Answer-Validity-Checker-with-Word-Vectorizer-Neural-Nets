{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz/N3SzQRDxQnpK/w2b75P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaledai069/Answer-Validity-Checker-with-Word-Vectorizer-Neural-Nets/blob/master/Alternative_Solution_Ranker_Vectorizer_Trainer_%26_Dataset_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jFBzoL4SAz1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2c3493-bf22-4a10-9222-a6862cd097e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentencepiece\n",
        "!pip install -q pyspellchecker\n",
        "!pip install -q h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7KoZGnovd9j",
        "outputId": "ff92d187-26bb-4182-86ba-bc351313079a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Imports"
      ],
      "metadata": {
        "id": "S-qbIx0Bmw2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import sentencepiece as spm\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "import h5py\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.corpus import treebank, brown\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "nltk.download('brown')\n",
        "nltk.download('treebank')"
      ],
      "metadata": {
        "id": "8mGhIqyRnewm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7eee563-b8cf-492e-b61f-ddca70fa25a6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Answer-Dataset"
      ],
      "metadata": {
        "id": "ylsyVKaUnQqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_list_path = \"/content/gdrive/MyDrive/First Pass Model/all_answer_list.tsv\"\n",
        "\n",
        "with open(answer_list_path, 'r') as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "answer_list = []\n",
        "for i in tqdm(range(len(lines)), ncols = 100):\n",
        "  pattern_omitted_word = re.sub('[~0-9\\n]', '', re.sub(r'\\t\"\"\"\"\"\"\\t', '', lines[i]))\n",
        "  answer_format = pattern_omitted_word.replace(' ', '')\n",
        "  answer_list.append(answer_format)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EBqmAPcgFFR",
        "outputId": "711da182-bd5d-48ec-ffb9-ca54b91d5b27"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████████████████| 458929/458929 [00:01<00:00, 251060.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/corpus.txt\", 'w') as f:\n",
        "  f.write(\"\\n\".join(answer_list))"
      ],
      "metadata": {
        "id": "N77MAtuDsC-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Sentence-Piece Tokenizer for sub-word recognization"
      ],
      "metadata": {
        "id": "7Xqclc_GnYgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training sub-word tokenizer\n",
        "# sp = spm.SentencePieceTrainer.Train(\"--input=/content/corpus.txt --model_prefix=sp_model --vocab_size=3000\")\n",
        "\n",
        "# loding trained sp-model for sub-word tokenized words\n",
        "sp_model = spm.SentencePieceProcessor()\n",
        "sp_model.Load(\"/content/sp_model.model\")"
      ],
      "metadata": {
        "id": "yUON3IpUq1jG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80669923-b9fb-45a7-8bf5-36890bf6ef62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Post-processing output string from the SP model"
      ],
      "metadata": {
        "id": "wcaXQEzxnmjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_word_answers_list = []\n",
        "for answer in tqdm(answer_list, ncols = 100):\n",
        "  sub_words = sp_model.EncodeAsPieces(answer)\n",
        "  try:\n",
        "    if len(sub_words[0]) > 1:\n",
        "      sub_words[0] = sub_words[0][1:]\n",
        "    elif len(sub_words[0]) == 1:\n",
        "      sub_words = sub_words[1:]\n",
        "    sub_word_answers_list.append(sub_words)\n",
        "  except IndexError:\n",
        "    print(answer, sub_words)\n",
        "\n",
        "print(len(sub_word_answers_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js3O8LUQt3MN",
        "outputId": "3ebe8926-1430-44d6-89ca-b28855e3c41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|███████████▍                                       | 103150/458929 [00:00<00:03, 106615.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██████████████                                     | 126963/458929 [00:01<00:03, 110480.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████████████████| 458929/458929 [00:04<00:00, 107975.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "458927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_embedding(answer, sp_model, word_vec_model):\n",
        "  test_sub_words = sp_model.EncodeAsPieces(answer)\n",
        "  # just a part of preprocessing step\n",
        "  if len(test_sub_words[0]) > 1:\n",
        "    test_sub_words[0] = test_sub_words[0][1:]\n",
        "  elif len(test_sub_words[0]) == 1:\n",
        "    test_sub_words = test_sub_words[1:]\n",
        "\n",
        "  embeddings = [word_vec_model.wv[word] for word in test_sub_words if word in word_vec_model.wv]\n",
        "  if embeddings:\n",
        "    final_word_vector = np.mean(embeddings, axis=0)\n",
        "  else:\n",
        "    final_word_vector = np.zeros(100)\n",
        "\n",
        "  return final_word_vector"
      ],
      "metadata": {
        "id": "1cpLDq6W3Y4s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vec_model = Word2Vec.load(\"/content/word2vec_model\")"
      ],
      "metadata": {
        "id": "ApuyC4Be1h_o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Partial-Dataset with only Positive-Answer listing"
      ],
      "metadata": {
        "id": "lCRlMQbznucz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_as_list = []\n",
        "for i, answer in tqdm(enumerate(answer_list), ncols = 100):\n",
        "  try:\n",
        "    dataset_as_list.append((answer, generate_answer_embedding(answer, sp_model, word_vec_model), 1))\n",
        "  except IndexError:\n",
        "    print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd9bjzTc4R1o",
        "outputId": "d130e195-f00f-4272-b742-2becbea5fbbb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "83162it [00:03, 25107.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "111446it [00:04, 24479.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "458929it [00:19, 23028.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dataset_as_list, columns = [\"answer\", \"embedding\", \"label\"])\n",
        "df.to_csv(\"/content/partial_dataset.csv\")"
      ],
      "metadata": {
        "id": "YVVIrZAJ1oEZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching Unique Words from mini-corpus\n",
        "- 'brown'\n",
        "- 'treebank'\n",
        "- 'wiki-2'"
      ],
      "metadata": {
        "id": "SRGtkjm9jBtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_all_alphabet(input_string):\n",
        "    pattern = '^[a-z]+$'\n",
        "    match = re.match(pattern, input_string.lower())\n",
        "    return match is not None"
      ],
      "metadata": {
        "id": "LLS-2qGuaVYP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using 'Penn Treebank', 'Brown' & 'Wiki-2' Corpus to build unique word repo to be added as positive samples"
      ],
      "metadata": {
        "id": "8qPdpAnmn0bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetching all the words from the treebank corpus\n",
        "tagged_words = treebank.tagged_words()\n",
        "\n",
        "words_list = []\n",
        "for (word, tag) in tagged_words:\n",
        "  hypen_words = word.split('-')\n",
        "  if len(hypen_words) > 1:\n",
        "    for hy_word in hypen_words:\n",
        "      if is_all_alphabet(hy_word):\n",
        "        words_list.append(hy_word.lower())\n",
        "  else:\n",
        "    if is_all_alphabet(word):\n",
        "      words_list.append(word.lower())\n",
        "\n",
        "words_list = [word for word in words_list if len(word) >= 3 and len(word) < 28]"
      ],
      "metadata": {
        "id": "UWc7wEU2JdIZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetching all the words from the brown corpus\n",
        "categories = brown.categories()\n",
        "brown_words_list = []\n",
        "for category in categories:\n",
        "  sentences = brown.sents(categories = category)\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if is_all_alphabet(word):\n",
        "        brown_words_list.append(word.lower())\n",
        "\n",
        "brown_words_list = [word for word in brown_words_list if len(word) >= 3 and len(word) < 28]\n",
        "all_words_list = words_list + brown_words_list\n",
        "print(\"Total number of words: \", len(all_words_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtVFHTPXVdiD",
        "outputId": "4966fb89-f9f3-430f-eae8-ea1f5d2fee3b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words:  846819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_df = pd.DataFrame(all_words_list, columns = ['Word'])\n",
        "words_df.drop_duplicates(subset = 'Word', keep = 'first', inplace = True)\n",
        "print(\"Total number of unique words from 'brown' & 'treebank'\", len(words_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv1vmGEpXrb3",
        "outputId": "2525dcc9-b956-405f-a2c0-5c079b404250"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique words from 'brown' & 'treebank' 41780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_words_from_set(set_path):\n",
        "  wiki_words_list = []\n",
        "  with open(set_path, 'r') as file:\n",
        "    all_lines = file.readlines()\n",
        "\n",
        "  word_count = 0\n",
        "  for line in all_lines:\n",
        "    all_words = line.split(' ')\n",
        "    for word in all_words:\n",
        "      if is_all_alphabet(word):\n",
        "        wiki_words_list.append(word.lower())\n",
        "\n",
        "  wiki_words_list = [word for word in wiki_words_list if len(word) >= 3 and len(word) < 28]\n",
        "  return wiki_words_list\n",
        "\n",
        "wiki_train_list = fetch_words_from_set(\"/content/wiki.train.raw\")\n",
        "wiki_valid_list = fetch_words_from_set(\"/content/wiki.valid.raw\")\n",
        "wiki_test_list = fetch_words_from_set(\"/content/wiki.test.raw\")\n",
        "all_wiki_words = wiki_train_list + wiki_valid_list + wiki_test_list\n",
        "\n",
        "print(\"Total number of words in the 'wiki-2': \", len(all_wiki_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VGz5j0uZZDo",
        "outputId": "b44bb8cf-2e42-4b88-a25b-e2682708f4a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in the 'wiki-2':  1662599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words_list += all_wiki_words\n",
        "words_df = pd.DataFrame(all_words_list, columns = ['Word'])\n",
        "words_df.drop_duplicates(subset = 'Word', keep = 'first', inplace = True)\n",
        "print(\"Total number of unique words from 'brown', 'treebank' & 'wiki-2' Dataset: \", len(words_df))\n",
        "\n",
        "# words_df.to_csv(\"/content/unique_answer_list.txt\", header=False, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyTANGsucIXW",
        "outputId": "8b4623ee-be83-4403-f5fe-5131f57c2f1d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique words from 'brown', 'treebank' & 'wiki-2' Dataset:  81181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postive and Negative sample generation"
      ],
      "metadata": {
        "id": "RYNjPB9ojMGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# modification to be applied to positive answer to generate negative answers\n",
        "\n",
        "def replace_random_chars(input_string, num_chars_to_replace):\n",
        "    positions_to_replace = random.sample(range(len(input_string)), num_chars_to_replace)\n",
        "\n",
        "    replaced_string = list(input_string)\n",
        "    for position in positions_to_replace:\n",
        "        replaced_string[position] = random.choice(string.ascii_letters.lower())\n",
        "\n",
        "    return ''.join(replaced_string)\n",
        "\n",
        "# random character omission from the input string\n",
        "\n",
        "def remove_random_chars(input_string, num_chars_to_remove):\n",
        "  positions_to_remove = random.sample(range(len(input_string)), 1)\n",
        "\n",
        "  for i, pos in enumerate(positions_to_remove):\n",
        "    input_string = input_string[:pos - i] + input_string[pos - i + 1 :]\n",
        "\n",
        "  return input_string"
      ],
      "metadata": {
        "id": "lW1l2Rchk_rf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_exist_of_word(word, ans_list):\n",
        "  return word in ans_list"
      ],
      "metadata": {
        "id": "z4Jf1KVOIAT8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if the answer is a single valid word\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "dataset_list = []\n",
        "\n",
        "for answer in tqdm(answer_list, ncols = 120):\n",
        "  if answer != '':\n",
        "    # original single answer\n",
        "    dataset_list.append((answer, generate_answer_embedding(answer, sp_model, word_vec_model), 1))\n",
        "\n",
        "    is_valid_word = spell.known([answer])\n",
        "\n",
        "    # generating negative answers with single valid words\n",
        "    if is_valid_word:\n",
        "      # get two negative answers for single valid word\n",
        "      for _ in range(2):\n",
        "        if len(answer) >= 10:\n",
        "          # concurrent two letter replacement\n",
        "          neg_answer_with_replace = replace_random_chars(answer, 2 )\n",
        "\n",
        "        else:\n",
        "          # a single letter replacement\n",
        "          neg_answer_with_replace = replace_random_chars(answer, 1)\n",
        "        dataset_list.append((neg_answer_with_replace, generate_answer_embedding(neg_answer_with_replace, sp_model, word_vec_model), 0))\n",
        "\n",
        "      if len(answer) > 5:\n",
        "        neg_answer_with_omission = remove_random_chars(answer, 1)\n",
        "        dataset_list.append((neg_answer_with_omission, generate_answer_embedding(neg_answer_with_omission, sp_model, word_vec_model), 0))\n",
        "\n",
        "    # generate negative answers with unsegmented answers\n",
        "    else:\n",
        "      neg_answer_with_replace_list = []\n",
        "      neg_answer_with_omission_list = []\n",
        "\n",
        "      for _ in range(2):\n",
        "        if len(answer) >= 15:\n",
        "          neg_answer_with_replace_list.append(replace_random_chars(answer, 3))\n",
        "          neg_answer_with_omission_list.append(remove_random_chars(answer, 2))\n",
        "\n",
        "        if len(answer) >= 10:\n",
        "          neg_answer_with_replace_list.append(replace_random_chars(answer, 2))\n",
        "          neg_answer_with_omission_list.append(remove_random_chars(answer, 1))\n",
        "\n",
        "        if len(answer) >= 5:\n",
        "          neg_answer_with_replace_list.append(replace_random_chars(answer, 1))\n",
        "          neg_answer_with_omission_list.append(remove_random_chars(answer, 1))\n",
        "\n",
        "      for neg_answer in neg_answer_with_replace_list + neg_answer_with_omission_list:\n",
        "        dataset_list.append((neg_answer, generate_answer_embedding(neg_answer, sp_model, word_vec_model), 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9UTN6Fhk47e",
        "outputId": "9eed2ac5-eda1-4924-f516-41f401ff8d83"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████| 458929/458929 [02:54<00:00, 2631.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in tqdm(words_df['Word'], ncols = 120):\n",
        "  dataset_list.append((word, generate_answer_embedding(word, sp_model, word_vec_model), 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsvDnK_VH1B9",
        "outputId": "34d3529f-4f6a-4f48-b087-c373cb387857"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████| 81181/81181 [00:02<00:00, 27802.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = pd.DataFrame(dataset_list, columns = ['Answer', 'Embedding', 'Label'])\n",
        "\n",
        "print(\"Dataset size before removing duplicates: \", len(dataset_df))\n",
        "dataset_df.drop_duplicates(subset = 'Answer', keep = 'first', inplace = True)\n",
        "print(\"Dataset size after removing duplicates: \", len(dataset_df))\n",
        "\n",
        "# dataset_df.to_csv(\"/content/answer dataset.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnU8XfsMNJiH",
        "outputId": "106af752-c27d-4347-ce52-968b4667e1a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size before removing duplicates:  3308843\n",
            "Dataset size after removing duplicates:  2971683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_data = np.array(dataset_df['Embedding'].tolist())\n",
        "target_data = np.array(dataset_df['Label'].tolist())\n",
        "\n",
        "assert features_data.shape[0] == target_data.shape[0]"
      ],
      "metadata": {
        "id": "ZTYuRFr96-8Z"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File('/content/answer_dataset.h5', 'w') as hdf:\n",
        "    hdf.create_dataset('Embedding', data = features_data)\n",
        "    hdf.create_dataset('Label', data = target_data)"
      ],
      "metadata": {
        "id": "ALKi2iLe66zS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Copying final-dataset to the G-Drive"
      ],
      "metadata": {
        "id": "aYgoXgXKoC8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "src_path = \"/content/answer_dataset.h5\"\n",
        "dest_path = \"/content/gdrive/MyDrive\"\n",
        "shutil.copy(src_path, dest_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pq0Nzov6ZgN_",
        "outputId": "7f8347e0-a81c-49bf-89ee-582080464a95"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/answer_dataset.h5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}