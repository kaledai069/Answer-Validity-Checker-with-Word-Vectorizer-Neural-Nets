{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmj5OB8oXPWqSAIwTkWA8E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaledai069/Answer-Validity-Checker-with-Word-Vectorizer-Neural-Nets/blob/master/Alternative_Solution_Ranker_Training_Neural_Net_Head.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvP4VSjtq0vk",
        "outputId": "5303ceb6-2cd3-45b6-f434-66e1e63936d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q h5py"
      ],
      "metadata": {
        "id": "LZwIKeeN-m0k"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import ast\n",
        "import h5py\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "xkw4JKrrrcHG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/gdrive/MyDrive/answer_dataset.h5\"\n",
        "\n",
        "with h5py.File(DATASET_PATH, 'r') as hdf:\n",
        "    features_loaded = hdf['Embedding'][:]\n",
        "    target_loaded = hdf['Label'][:]"
      ],
      "metadata": {
        "id": "D6IuQ3zD-1qT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_loaded.shape, target_loaded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOpJW_v7BgDM",
        "outputId": "ef907620-ef1b-4b83-e89e-0cdf8ebb1f22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2971683, 128), (2971683,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seperating +ve and -ve features\n",
        "positive_samples = features_loaded[target_loaded == 1]\n",
        "negative_samples = features_loaded[target_loaded == 0]\n",
        "\n",
        "'''\n",
        "A testing section hereby commences\n",
        "'''\n",
        "# negative_samples = negative_samples[:len(positive_samples)]\n",
        "\n",
        "# train, validation and test set split for the negative samples\n",
        "# splitting negative samples in the ratio 70-15-15\n",
        "train_neg, temp_neg = train_test_split(negative_samples, test_size = 0.1, random_state = 69)\n",
        "valid_neg, test_neg = train_test_split(temp_neg, test_size = 0.5, random_state = 69)\n",
        "\n",
        "# train, validation and test set split for the positive samples\n",
        "# splitting positive samples in the ratio of 90-5-5\n",
        "train_posi, temp_posi = train_test_split(positive_samples, test_size = 0.1, random_state = 69)\n",
        "valid_posi, test_posi = train_test_split(temp_posi, test_size = 0.5, random_state = 69)\n",
        "\n",
        "# preparing features vertical stack with positive and negative ratios\n",
        "train_features = np.vstack([train_neg, train_posi])\n",
        "valid_features = np.vstack([valid_neg, valid_posi])\n",
        "test_features = np.vstack([test_neg, test_posi])\n",
        "\n",
        "# target labels for negative and positive samples correspondingly\n",
        "train_labels = np.concatenate([np.zeros(len(train_neg)), np.ones(len(train_posi))])\n",
        "valid_labels = np.concatenate([np.zeros(len(valid_neg)), np.ones(len(valid_posi))])\n",
        "test_labels = np.concatenate([np.zeros(len(test_neg)), np.ones(len(test_posi))])\n",
        "\n",
        "# obvious shuffle\n",
        "shuffle_train = np.random.permutation(len(train_features))\n",
        "shuffle_valid = np.random.permutation(len(valid_features))\n",
        "shuffle_test = np.random.permutation(len(test_features))\n",
        "\n",
        "# Actual Train-Validation-Test Split\n",
        "X_train = torch.Tensor(train_features[shuffle_train])\n",
        "Y_train = torch.FloatTensor(train_labels[shuffle_train])\n",
        "\n",
        "X_valid = torch.Tensor(valid_features[shuffle_valid])\n",
        "Y_valid = torch.FloatTensor(valid_labels[shuffle_valid])\n",
        "\n",
        "X_test = torch.Tensor(test_features[shuffle_test])\n",
        "Y_test = torch.FloatTensor(test_labels[shuffle_test])"
      ],
      "metadata": {
        "id": "5ZqWQxiFB7Xq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# GPU intensive training support\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# data loaded to the active device (CPU or GPU)\n",
        "X_train_tensor, y_train_tensor = X_train.to(device), Y_train.to(device)\n",
        "X_valid_tensor, y_valid_tensor = X_valid.to(device), Y_valid.to(device)\n",
        "\n",
        "# tensor dataset loader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "\n",
        "# dataLoader Module\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle = False)"
      ],
      "metadata": {
        "id": "RKRs1pBTFWWk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handling negative sample skewness with BCELogitLoss (pos_weight)\n",
        "num_positive_samples = len(y_train_tensor[y_train_tensor == 1])\n",
        "num_negative_samples = len(y_train_tensor[y_train_tensor == 0])\n",
        "\n",
        "total_num_samples = num_positive_samples + num_negative_samples\n",
        "\n",
        "weight_for_positive = total_num_samples / (2 * num_positive_samples)\n",
        "weight_for_negative = total_num_samples / (2 * num_negative_samples)\n",
        "\n",
        "class_weights = torch.tensor([weight_for_negative], dtype = torch.float32)\n",
        "print(class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODQUfUDIF1c8",
        "outputId": "33509a6b-a073-4e26-d39a-f8419c64d1d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.1997])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom neural net with\n",
        "class AnswerValidator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AnswerValidator, self).__init__()\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.output = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.relu3(self.fc3(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "model = AnswerValidator().to(device)\n",
        "\n",
        "# Binary cross entropy (BCE) as the loss function with optmizer used as Adam\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight = class_weights.to(device))\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "print(\"Total no. of parameters in the model: \", sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "id": "tDNIWN47x4mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f0c7bc-e9c2-451b-f5c3-5f2af25c19af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of parameters in the model:  10881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main train_loop\n",
        "num_epochs = 10\n",
        "train_eval_interval = 1000\n",
        "valid_eval_interval = 50000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_train = 0\n",
        "    train_data_count = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate outputs and apply sigmoid\n",
        "        outputs = torch.sigmoid(model(inputs)).squeeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # print(\"Output: \", outputs, \"\\nLabels: \", labels)\n",
        "        # break\n",
        "\n",
        "        predictions = outputs > 0.5\n",
        "        correct_train += torch.sum(predictions == labels.byte()).item()\n",
        "        train_data_count += len(inputs)\n",
        "\n",
        "        # Compute training accuracy every train_eval_interval iterations\n",
        "        if i % train_eval_interval == 0:\n",
        "            avg_train_loss = total_loss / train_eval_interval\n",
        "\n",
        "            print(f'Iteration {i}, '\n",
        "                  f'\\t Training Loss: {avg_train_loss:.6f}, \\t Training Accuracy: {correct_train / train_data_count:.6f}')\n",
        "\n",
        "            total_loss = 0.0\n",
        "            # correct_train = 0\n",
        "\n",
        "        # Validation every valid_eval_interval iterations\n",
        "        if i % valid_eval_interval == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                valid_loss = 0.0\n",
        "                correct_valid = 0\n",
        "                for inputs, labels in valid_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = torch.sigmoid(model(inputs)).squeeze(1)\n",
        "                    valid_loss += criterion(outputs, labels).item()\n",
        "\n",
        "                    # Compute validation accuracy\n",
        "                    predictions = outputs > 0.5\n",
        "                    correct_valid += torch.sum(predictions == labels.byte()).item()\n",
        "\n",
        "                avg_valid_loss = valid_loss / len(valid_loader)\n",
        "                accuracy_valid = correct_valid / len(valid_dataset)\n",
        "\n",
        "                print(f'\\n VALIDATION: Iteration {i}, '\n",
        "                      f'Validation Loss: {avg_valid_loss:.6f}, \\t Validation Accuracy: {accuracy_valid:.6f}\\n')\n",
        "\n",
        "            model.train()\n",
        "\n",
        "    # Final epoch summary\n",
        "    print(f'Final Epoch {num_epochs}, '\n",
        "          f'Training Loss: {avg_train_loss:.4f}, Training Accuracy: {correct_train / (i * len(inputs)):.4f}')\n"
      ],
      "metadata": {
        "id": "pHBoc_DNIcO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44a930a-8b69-40d7-c626-a3520905e001"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1000, \t Training Loss: 0.920072, \t Training Accuracy: 0.847437\n",
            "Iteration 2000, \t Training Loss: 0.904891, \t Training Accuracy: 0.848219\n",
            "Iteration 3000, \t Training Loss: 0.902406, \t Training Accuracy: 0.849292\n",
            "Iteration 4000, \t Training Loss: 0.905682, \t Training Accuracy: 0.849164\n",
            "Iteration 5000, \t Training Loss: 0.902933, \t Training Accuracy: 0.849550\n",
            "Iteration 6000, \t Training Loss: 0.898696, \t Training Accuracy: 0.850193\n",
            "Iteration 7000, \t Training Loss: 0.904459, \t Training Accuracy: 0.849672\n",
            "Iteration 8000, \t Training Loss: 0.897455, \t Training Accuracy: 0.850209\n",
            "Iteration 9000, \t Training Loss: 0.903156, \t Training Accuracy: 0.850175\n",
            "Iteration 10000, \t Training Loss: 0.901691, \t Training Accuracy: 0.850181\n",
            "Iteration 11000, \t Training Loss: 0.900223, \t Training Accuracy: 0.850190\n",
            "Iteration 12000, \t Training Loss: 0.899658, \t Training Accuracy: 0.850273\n",
            "Iteration 13000, \t Training Loss: 0.896487, \t Training Accuracy: 0.850623\n",
            "Iteration 14000, \t Training Loss: 0.893549, \t Training Accuracy: 0.850940\n",
            "Iteration 15000, \t Training Loss: 0.896332, \t Training Accuracy: 0.851147\n",
            "Iteration 16000, \t Training Loss: 0.895020, \t Training Accuracy: 0.851273\n",
            "Iteration 17000, \t Training Loss: 0.898317, \t Training Accuracy: 0.851240\n",
            "Iteration 18000, \t Training Loss: 0.895706, \t Training Accuracy: 0.851346\n",
            "Iteration 19000, \t Training Loss: 0.898482, \t Training Accuracy: 0.851204\n",
            "Iteration 20000, \t Training Loss: 0.893047, \t Training Accuracy: 0.851407\n",
            "Iteration 21000, \t Training Loss: 0.895771, \t Training Accuracy: 0.851432\n",
            "Iteration 22000, \t Training Loss: 0.893713, \t Training Accuracy: 0.851585\n",
            "Iteration 23000, \t Training Loss: 0.899460, \t Training Accuracy: 0.851537\n",
            "Iteration 24000, \t Training Loss: 0.896082, \t Training Accuracy: 0.851566\n",
            "Iteration 25000, \t Training Loss: 0.893953, \t Training Accuracy: 0.851636\n",
            "Iteration 26000, \t Training Loss: 0.893713, \t Training Accuracy: 0.851725\n",
            "Iteration 27000, \t Training Loss: 0.893901, \t Training Accuracy: 0.851838\n",
            "Iteration 28000, \t Training Loss: 0.893286, \t Training Accuracy: 0.851962\n",
            "Iteration 29000, \t Training Loss: 0.896866, \t Training Accuracy: 0.851886\n",
            "Iteration 30000, \t Training Loss: 0.895019, \t Training Accuracy: 0.851876\n",
            "Iteration 31000, \t Training Loss: 0.898522, \t Training Accuracy: 0.851830\n",
            "Iteration 32000, \t Training Loss: 0.894372, \t Training Accuracy: 0.851827\n",
            "Iteration 33000, \t Training Loss: 0.889515, \t Training Accuracy: 0.851913\n",
            "Iteration 34000, \t Training Loss: 0.892059, \t Training Accuracy: 0.851998\n",
            "Iteration 35000, \t Training Loss: 0.891333, \t Training Accuracy: 0.852066\n",
            "Iteration 36000, \t Training Loss: 0.891452, \t Training Accuracy: 0.852105\n",
            "Iteration 37000, \t Training Loss: 0.892150, \t Training Accuracy: 0.852120\n",
            "Iteration 38000, \t Training Loss: 0.891755, \t Training Accuracy: 0.852176\n",
            "Iteration 39000, \t Training Loss: 0.890651, \t Training Accuracy: 0.852206\n",
            "Iteration 40000, \t Training Loss: 0.894219, \t Training Accuracy: 0.852170\n",
            "Iteration 41000, \t Training Loss: 0.890924, \t Training Accuracy: 0.852210\n",
            "Final Epoch 10, Training Loss: 0.8909, Training Accuracy: 3.0303\n",
            "Iteration 1000, \t Training Loss: 0.890789, \t Training Accuracy: 0.854844\n",
            "Iteration 2000, \t Training Loss: 0.887528, \t Training Accuracy: 0.855547\n",
            "Iteration 3000, \t Training Loss: 0.892462, \t Training Accuracy: 0.854500\n",
            "Iteration 4000, \t Training Loss: 0.891555, \t Training Accuracy: 0.853973\n",
            "Iteration 5000, \t Training Loss: 0.890085, \t Training Accuracy: 0.854163\n",
            "Iteration 6000, \t Training Loss: 0.890662, \t Training Accuracy: 0.853966\n",
            "Iteration 7000, \t Training Loss: 0.891922, \t Training Accuracy: 0.853768\n",
            "Iteration 8000, \t Training Loss: 0.889191, \t Training Accuracy: 0.853986\n",
            "Iteration 9000, \t Training Loss: 0.893999, \t Training Accuracy: 0.853753\n",
            "Iteration 10000, \t Training Loss: 0.890976, \t Training Accuracy: 0.853614\n",
            "Iteration 11000, \t Training Loss: 0.890924, \t Training Accuracy: 0.853685\n",
            "Iteration 12000, \t Training Loss: 0.890169, \t Training Accuracy: 0.853754\n",
            "Iteration 13000, \t Training Loss: 0.891161, \t Training Accuracy: 0.853638\n",
            "Iteration 14000, \t Training Loss: 0.889709, \t Training Accuracy: 0.853556\n",
            "Iteration 15000, \t Training Loss: 0.892823, \t Training Accuracy: 0.853526\n",
            "Iteration 16000, \t Training Loss: 0.890425, \t Training Accuracy: 0.853460\n",
            "Iteration 17000, \t Training Loss: 0.890406, \t Training Accuracy: 0.853481\n",
            "Iteration 18000, \t Training Loss: 0.891448, \t Training Accuracy: 0.853433\n",
            "Iteration 19000, \t Training Loss: 0.889741, \t Training Accuracy: 0.853400\n",
            "Iteration 20000, \t Training Loss: 0.890420, \t Training Accuracy: 0.853340\n",
            "Iteration 21000, \t Training Loss: 0.890816, \t Training Accuracy: 0.853234\n",
            "Iteration 22000, \t Training Loss: 0.891999, \t Training Accuracy: 0.853107\n",
            "Iteration 23000, \t Training Loss: 0.892617, \t Training Accuracy: 0.852988\n",
            "Iteration 24000, \t Training Loss: 0.889232, \t Training Accuracy: 0.853029\n",
            "Iteration 25000, \t Training Loss: 0.890095, \t Training Accuracy: 0.853047\n",
            "Iteration 26000, \t Training Loss: 0.888281, \t Training Accuracy: 0.853013\n",
            "Iteration 27000, \t Training Loss: 0.892330, \t Training Accuracy: 0.852907\n",
            "Iteration 28000, \t Training Loss: 0.888725, \t Training Accuracy: 0.852981\n",
            "Iteration 29000, \t Training Loss: 0.890216, \t Training Accuracy: 0.852937\n",
            "Iteration 30000, \t Training Loss: 0.891177, \t Training Accuracy: 0.852860\n",
            "Iteration 31000, \t Training Loss: 0.883860, \t Training Accuracy: 0.853058\n",
            "Iteration 32000, \t Training Loss: 0.891374, \t Training Accuracy: 0.853067\n",
            "Iteration 33000, \t Training Loss: 0.887711, \t Training Accuracy: 0.853156\n",
            "Iteration 34000, \t Training Loss: 0.887575, \t Training Accuracy: 0.853199\n",
            "Iteration 35000, \t Training Loss: 0.890668, \t Training Accuracy: 0.853133\n",
            "Iteration 36000, \t Training Loss: 0.888201, \t Training Accuracy: 0.853200\n",
            "Iteration 37000, \t Training Loss: 0.886702, \t Training Accuracy: 0.853205\n",
            "Iteration 38000, \t Training Loss: 0.889113, \t Training Accuracy: 0.853231\n",
            "Iteration 39000, \t Training Loss: 0.888926, \t Training Accuracy: 0.853224\n",
            "Iteration 40000, \t Training Loss: 0.888602, \t Training Accuracy: 0.853228\n",
            "Iteration 41000, \t Training Loss: 0.891667, \t Training Accuracy: 0.853229\n",
            "Final Epoch 10, Training Loss: 0.8917, Training Accuracy: 3.0336\n",
            "Iteration 1000, \t Training Loss: 0.886829, \t Training Accuracy: 0.854187\n",
            "Iteration 2000, \t Training Loss: 0.887830, \t Training Accuracy: 0.853273\n",
            "Iteration 3000, \t Training Loss: 0.889994, \t Training Accuracy: 0.853057\n",
            "Iteration 4000, \t Training Loss: 0.887604, \t Training Accuracy: 0.853180\n",
            "Iteration 5000, \t Training Loss: 0.887686, \t Training Accuracy: 0.853634\n",
            "Iteration 6000, \t Training Loss: 0.888614, \t Training Accuracy: 0.853557\n",
            "Iteration 7000, \t Training Loss: 0.887294, \t Training Accuracy: 0.853431\n",
            "Iteration 8000, \t Training Loss: 0.888153, \t Training Accuracy: 0.853381\n",
            "Iteration 9000, \t Training Loss: 0.888251, \t Training Accuracy: 0.853385\n",
            "Iteration 10000, \t Training Loss: 0.887347, \t Training Accuracy: 0.853683\n",
            "Iteration 11000, \t Training Loss: 0.887048, \t Training Accuracy: 0.853764\n",
            "Iteration 12000, \t Training Loss: 0.888204, \t Training Accuracy: 0.853725\n",
            "Iteration 13000, \t Training Loss: 0.887193, \t Training Accuracy: 0.853837\n",
            "Iteration 14000, \t Training Loss: 0.888972, \t Training Accuracy: 0.853710\n",
            "Iteration 15000, \t Training Loss: 0.886362, \t Training Accuracy: 0.853775\n",
            "Iteration 16000, \t Training Loss: 0.885512, \t Training Accuracy: 0.853980\n",
            "Iteration 17000, \t Training Loss: 0.890243, \t Training Accuracy: 0.853947\n",
            "Iteration 18000, \t Training Loss: 0.887461, \t Training Accuracy: 0.853954\n",
            "Iteration 19000, \t Training Loss: 0.887444, \t Training Accuracy: 0.853948\n",
            "Iteration 20000, \t Training Loss: 0.886426, \t Training Accuracy: 0.853937\n",
            "Iteration 21000, \t Training Loss: 0.889758, \t Training Accuracy: 0.853955\n",
            "Iteration 22000, \t Training Loss: 0.888278, \t Training Accuracy: 0.853912\n",
            "Iteration 23000, \t Training Loss: 0.888623, \t Training Accuracy: 0.853882\n",
            "Iteration 24000, \t Training Loss: 0.885979, \t Training Accuracy: 0.853951\n",
            "Iteration 25000, \t Training Loss: 0.890198, \t Training Accuracy: 0.853806\n",
            "Iteration 26000, \t Training Loss: 0.890746, \t Training Accuracy: 0.853681\n",
            "Iteration 27000, \t Training Loss: 0.885622, \t Training Accuracy: 0.853717\n",
            "Iteration 28000, \t Training Loss: 0.888848, \t Training Accuracy: 0.853734\n",
            "Iteration 29000, \t Training Loss: 0.884684, \t Training Accuracy: 0.853781\n",
            "Iteration 30000, \t Training Loss: 0.883746, \t Training Accuracy: 0.853859\n",
            "Iteration 31000, \t Training Loss: 0.885021, \t Training Accuracy: 0.853872\n",
            "Iteration 32000, \t Training Loss: 0.886804, \t Training Accuracy: 0.853875\n",
            "Iteration 33000, \t Training Loss: 0.885749, \t Training Accuracy: 0.853936\n",
            "Iteration 34000, \t Training Loss: 0.887929, \t Training Accuracy: 0.853947\n",
            "Iteration 35000, \t Training Loss: 0.890588, \t Training Accuracy: 0.853838\n",
            "Iteration 36000, \t Training Loss: 0.886665, \t Training Accuracy: 0.853855\n",
            "Iteration 37000, \t Training Loss: 0.888744, \t Training Accuracy: 0.853838\n",
            "Iteration 38000, \t Training Loss: 0.886241, \t Training Accuracy: 0.853893\n",
            "Iteration 39000, \t Training Loss: 0.886643, \t Training Accuracy: 0.853894\n",
            "Iteration 40000, \t Training Loss: 0.887323, \t Training Accuracy: 0.853940\n",
            "Iteration 41000, \t Training Loss: 0.887484, \t Training Accuracy: 0.853960\n",
            "Final Epoch 10, Training Loss: 0.8875, Training Accuracy: 3.0363\n",
            "Iteration 1000, \t Training Loss: 0.885368, \t Training Accuracy: 0.855922\n",
            "Iteration 2000, \t Training Loss: 0.886670, \t Training Accuracy: 0.855133\n",
            "Iteration 3000, \t Training Loss: 0.884282, \t Training Accuracy: 0.855172\n",
            "Iteration 4000, \t Training Loss: 0.886313, \t Training Accuracy: 0.854926\n",
            "Iteration 5000, \t Training Loss: 0.885180, \t Training Accuracy: 0.854991\n",
            "Iteration 6000, \t Training Loss: 0.883912, \t Training Accuracy: 0.855240\n",
            "Iteration 7000, \t Training Loss: 0.885962, \t Training Accuracy: 0.855134\n",
            "Iteration 8000, \t Training Loss: 0.884825, \t Training Accuracy: 0.855121\n",
            "Iteration 9000, \t Training Loss: 0.884595, \t Training Accuracy: 0.855262\n",
            "Iteration 10000, \t Training Loss: 0.886920, \t Training Accuracy: 0.855137\n",
            "Iteration 11000, \t Training Loss: 0.883789, \t Training Accuracy: 0.855334\n",
            "Iteration 12000, \t Training Loss: 0.886362, \t Training Accuracy: 0.855366\n",
            "Iteration 13000, \t Training Loss: 0.881998, \t Training Accuracy: 0.855665\n",
            "Iteration 14000, \t Training Loss: 0.886783, \t Training Accuracy: 0.855435\n",
            "Iteration 15000, \t Training Loss: 0.884380, \t Training Accuracy: 0.855451\n",
            "Iteration 16000, \t Training Loss: 0.885235, \t Training Accuracy: 0.855571\n",
            "Iteration 17000, \t Training Loss: 0.884686, \t Training Accuracy: 0.855495\n",
            "Iteration 18000, \t Training Loss: 0.886008, \t Training Accuracy: 0.855384\n",
            "Iteration 19000, \t Training Loss: 0.886512, \t Training Accuracy: 0.855403\n",
            "Iteration 20000, \t Training Loss: 0.884469, \t Training Accuracy: 0.855373\n",
            "Iteration 21000, \t Training Loss: 0.888085, \t Training Accuracy: 0.855359\n",
            "Iteration 22000, \t Training Loss: 0.890040, \t Training Accuracy: 0.855085\n",
            "Iteration 23000, \t Training Loss: 0.886192, \t Training Accuracy: 0.855087\n",
            "Iteration 24000, \t Training Loss: 0.886032, \t Training Accuracy: 0.855023\n",
            "Iteration 25000, \t Training Loss: 0.887376, \t Training Accuracy: 0.855023\n",
            "Iteration 26000, \t Training Loss: 0.888020, \t Training Accuracy: 0.854829\n",
            "Iteration 27000, \t Training Loss: 0.883640, \t Training Accuracy: 0.854978\n",
            "Iteration 28000, \t Training Loss: 0.884195, \t Training Accuracy: 0.855035\n",
            "Iteration 29000, \t Training Loss: 0.884048, \t Training Accuracy: 0.855096\n",
            "Iteration 30000, \t Training Loss: 0.885505, \t Training Accuracy: 0.855095\n",
            "Iteration 31000, \t Training Loss: 0.886520, \t Training Accuracy: 0.855087\n",
            "Iteration 32000, \t Training Loss: 0.886191, \t Training Accuracy: 0.855079\n",
            "Iteration 33000, \t Training Loss: 0.886098, \t Training Accuracy: 0.855076\n",
            "Iteration 34000, \t Training Loss: 0.885792, \t Training Accuracy: 0.855078\n",
            "Iteration 35000, \t Training Loss: 0.884704, \t Training Accuracy: 0.855098\n",
            "Iteration 36000, \t Training Loss: 0.887386, \t Training Accuracy: 0.855021\n",
            "Iteration 37000, \t Training Loss: 0.883369, \t Training Accuracy: 0.855082\n",
            "Iteration 38000, \t Training Loss: 0.885124, \t Training Accuracy: 0.855085\n",
            "Iteration 39000, \t Training Loss: 0.886107, \t Training Accuracy: 0.855065\n",
            "Iteration 40000, \t Training Loss: 0.886571, \t Training Accuracy: 0.855045\n",
            "Iteration 41000, \t Training Loss: 0.885412, \t Training Accuracy: 0.855038\n",
            "Final Epoch 10, Training Loss: 0.8854, Training Accuracy: 3.0400\n",
            "Iteration 1000, \t Training Loss: 0.883363, \t Training Accuracy: 0.857109\n",
            "Iteration 2000, \t Training Loss: 0.885352, \t Training Accuracy: 0.855766\n",
            "Iteration 3000, \t Training Loss: 0.884225, \t Training Accuracy: 0.855458\n",
            "Iteration 4000, \t Training Loss: 0.883467, \t Training Accuracy: 0.856000\n",
            "Iteration 5000, \t Training Loss: 0.882731, \t Training Accuracy: 0.856472\n",
            "Iteration 6000, \t Training Loss: 0.882991, \t Training Accuracy: 0.856245\n",
            "Iteration 7000, \t Training Loss: 0.883734, \t Training Accuracy: 0.856109\n",
            "Iteration 8000, \t Training Loss: 0.882598, \t Training Accuracy: 0.856227\n",
            "Iteration 9000, \t Training Loss: 0.883167, \t Training Accuracy: 0.856391\n",
            "Iteration 10000, \t Training Loss: 0.886893, \t Training Accuracy: 0.856022\n",
            "Iteration 11000, \t Training Loss: 0.884423, \t Training Accuracy: 0.855743\n",
            "Iteration 12000, \t Training Loss: 0.883017, \t Training Accuracy: 0.855961\n",
            "Iteration 13000, \t Training Loss: 0.883208, \t Training Accuracy: 0.856026\n",
            "Iteration 14000, \t Training Loss: 0.884760, \t Training Accuracy: 0.855907\n",
            "Iteration 15000, \t Training Loss: 0.884299, \t Training Accuracy: 0.855955\n",
            "Iteration 16000, \t Training Loss: 0.885989, \t Training Accuracy: 0.855858\n",
            "Iteration 17000, \t Training Loss: 0.885042, \t Training Accuracy: 0.855799\n",
            "Iteration 18000, \t Training Loss: 0.881992, \t Training Accuracy: 0.855894\n",
            "Iteration 19000, \t Training Loss: 0.885357, \t Training Accuracy: 0.855840\n",
            "Iteration 20000, \t Training Loss: 0.886239, \t Training Accuracy: 0.855723\n",
            "Iteration 21000, \t Training Loss: 0.883842, \t Training Accuracy: 0.855718\n",
            "Iteration 22000, \t Training Loss: 0.881351, \t Training Accuracy: 0.855743\n",
            "Iteration 23000, \t Training Loss: 0.883536, \t Training Accuracy: 0.855854\n",
            "Iteration 24000, \t Training Loss: 0.884164, \t Training Accuracy: 0.855794\n",
            "Iteration 25000, \t Training Loss: 0.886049, \t Training Accuracy: 0.855686\n",
            "Iteration 26000, \t Training Loss: 0.884486, \t Training Accuracy: 0.855630\n",
            "Iteration 27000, \t Training Loss: 0.886809, \t Training Accuracy: 0.855530\n",
            "Iteration 28000, \t Training Loss: 0.885481, \t Training Accuracy: 0.855545\n",
            "Iteration 29000, \t Training Loss: 0.885189, \t Training Accuracy: 0.855546\n",
            "Iteration 30000, \t Training Loss: 0.888099, \t Training Accuracy: 0.855487\n",
            "Iteration 31000, \t Training Loss: 0.886140, \t Training Accuracy: 0.855477\n",
            "Iteration 32000, \t Training Loss: 0.885129, \t Training Accuracy: 0.855481\n",
            "Iteration 33000, \t Training Loss: 0.886282, \t Training Accuracy: 0.855395\n",
            "Iteration 34000, \t Training Loss: 0.885575, \t Training Accuracy: 0.855365\n",
            "Iteration 35000, \t Training Loss: 0.883288, \t Training Accuracy: 0.855419\n",
            "Iteration 36000, \t Training Loss: 0.882689, \t Training Accuracy: 0.855428\n",
            "Iteration 37000, \t Training Loss: 0.884923, \t Training Accuracy: 0.855352\n",
            "Iteration 38000, \t Training Loss: 0.882750, \t Training Accuracy: 0.855350\n",
            "Iteration 39000, \t Training Loss: 0.886313, \t Training Accuracy: 0.855309\n",
            "Iteration 40000, \t Training Loss: 0.884152, \t Training Accuracy: 0.855306\n",
            "Iteration 41000, \t Training Loss: 0.885442, \t Training Accuracy: 0.855255\n",
            "Final Epoch 10, Training Loss: 0.8854, Training Accuracy: 3.0410\n",
            "Iteration 1000, \t Training Loss: 0.887170, \t Training Accuracy: 0.852016\n",
            "Iteration 2000, \t Training Loss: 0.884220, \t Training Accuracy: 0.854453\n",
            "Iteration 3000, \t Training Loss: 0.883290, \t Training Accuracy: 0.855151\n",
            "Iteration 4000, \t Training Loss: 0.882321, \t Training Accuracy: 0.855871\n",
            "Iteration 5000, \t Training Loss: 0.883090, \t Training Accuracy: 0.855972\n",
            "Iteration 6000, \t Training Loss: 0.882044, \t Training Accuracy: 0.855956\n",
            "Iteration 7000, \t Training Loss: 0.884331, \t Training Accuracy: 0.855962\n",
            "Iteration 8000, \t Training Loss: 0.886351, \t Training Accuracy: 0.855684\n",
            "Iteration 9000, \t Training Loss: 0.886073, \t Training Accuracy: 0.855465\n",
            "Iteration 10000, \t Training Loss: 0.881799, \t Training Accuracy: 0.855337\n",
            "Iteration 11000, \t Training Loss: 0.882449, \t Training Accuracy: 0.855618\n",
            "Iteration 12000, \t Training Loss: 0.878688, \t Training Accuracy: 0.856081\n",
            "Iteration 13000, \t Training Loss: 0.880630, \t Training Accuracy: 0.856296\n",
            "Iteration 14000, \t Training Loss: 0.882776, \t Training Accuracy: 0.856400\n",
            "Iteration 15000, \t Training Loss: 0.883458, \t Training Accuracy: 0.856389\n",
            "Iteration 16000, \t Training Loss: 0.883572, \t Training Accuracy: 0.856493\n",
            "Iteration 17000, \t Training Loss: 0.889391, \t Training Accuracy: 0.856325\n",
            "Iteration 18000, \t Training Loss: 0.883632, \t Training Accuracy: 0.856344\n",
            "Iteration 19000, \t Training Loss: 0.884111, \t Training Accuracy: 0.856377\n",
            "Iteration 20000, \t Training Loss: 0.884015, \t Training Accuracy: 0.856274\n",
            "Iteration 21000, \t Training Loss: 0.885314, \t Training Accuracy: 0.856126\n",
            "Iteration 22000, \t Training Loss: 0.881662, \t Training Accuracy: 0.856239\n",
            "Iteration 23000, \t Training Loss: 0.884427, \t Training Accuracy: 0.856231\n",
            "Iteration 24000, \t Training Loss: 0.883993, \t Training Accuracy: 0.856223\n",
            "Iteration 25000, \t Training Loss: 0.887324, \t Training Accuracy: 0.856084\n",
            "Iteration 26000, \t Training Loss: 0.881092, \t Training Accuracy: 0.856123\n",
            "Iteration 27000, \t Training Loss: 0.882005, \t Training Accuracy: 0.856225\n",
            "Iteration 28000, \t Training Loss: 0.883607, \t Training Accuracy: 0.856208\n",
            "Iteration 29000, \t Training Loss: 0.883683, \t Training Accuracy: 0.856235\n",
            "Iteration 30000, \t Training Loss: 0.884040, \t Training Accuracy: 0.856247\n",
            "Iteration 31000, \t Training Loss: 0.883965, \t Training Accuracy: 0.856242\n",
            "Iteration 32000, \t Training Loss: 0.880713, \t Training Accuracy: 0.856275\n",
            "Iteration 33000, \t Training Loss: 0.881634, \t Training Accuracy: 0.856320\n",
            "Iteration 34000, \t Training Loss: 0.885300, \t Training Accuracy: 0.856273\n",
            "Iteration 35000, \t Training Loss: 0.884519, \t Training Accuracy: 0.856222\n",
            "Iteration 36000, \t Training Loss: 0.880968, \t Training Accuracy: 0.856267\n",
            "Iteration 37000, \t Training Loss: 0.884057, \t Training Accuracy: 0.856227\n",
            "Iteration 38000, \t Training Loss: 0.882767, \t Training Accuracy: 0.856241\n",
            "Iteration 39000, \t Training Loss: 0.883721, \t Training Accuracy: 0.856226\n",
            "Iteration 40000, \t Training Loss: 0.887312, \t Training Accuracy: 0.856182\n",
            "Iteration 41000, \t Training Loss: 0.881916, \t Training Accuracy: 0.856261\n",
            "Final Epoch 10, Training Loss: 0.8819, Training Accuracy: 3.0446\n",
            "Iteration 1000, \t Training Loss: 0.881527, \t Training Accuracy: 0.858734\n",
            "Iteration 2000, \t Training Loss: 0.883650, \t Training Accuracy: 0.858508\n",
            "Iteration 3000, \t Training Loss: 0.882902, \t Training Accuracy: 0.858938\n",
            "Iteration 4000, \t Training Loss: 0.884064, \t Training Accuracy: 0.857961\n",
            "Iteration 5000, \t Training Loss: 0.884288, \t Training Accuracy: 0.858106\n",
            "Iteration 6000, \t Training Loss: 0.882611, \t Training Accuracy: 0.857958\n",
            "Iteration 7000, \t Training Loss: 0.883711, \t Training Accuracy: 0.857792\n",
            "Iteration 8000, \t Training Loss: 0.882581, \t Training Accuracy: 0.857723\n",
            "Iteration 9000, \t Training Loss: 0.882610, \t Training Accuracy: 0.857757\n",
            "Iteration 10000, \t Training Loss: 0.881057, \t Training Accuracy: 0.857975\n",
            "Iteration 11000, \t Training Loss: 0.882926, \t Training Accuracy: 0.857690\n",
            "Iteration 12000, \t Training Loss: 0.887191, \t Training Accuracy: 0.857323\n",
            "Iteration 13000, \t Training Loss: 0.881855, \t Training Accuracy: 0.857281\n",
            "Iteration 14000, \t Training Loss: 0.883004, \t Training Accuracy: 0.857241\n",
            "Iteration 15000, \t Training Loss: 0.887323, \t Training Accuracy: 0.856973\n",
            "Iteration 16000, \t Training Loss: 0.883062, \t Training Accuracy: 0.856976\n",
            "Iteration 17000, \t Training Loss: 0.882157, \t Training Accuracy: 0.857004\n",
            "Iteration 18000, \t Training Loss: 0.881257, \t Training Accuracy: 0.857048\n",
            "Iteration 19000, \t Training Loss: 0.883482, \t Training Accuracy: 0.857083\n",
            "Iteration 20000, \t Training Loss: 0.882584, \t Training Accuracy: 0.856973\n",
            "Iteration 21000, \t Training Loss: 0.883594, \t Training Accuracy: 0.856894\n",
            "Iteration 22000, \t Training Loss: 0.881463, \t Training Accuracy: 0.857001\n",
            "Iteration 23000, \t Training Loss: 0.881869, \t Training Accuracy: 0.857031\n",
            "Iteration 24000, \t Training Loss: 0.883027, \t Training Accuracy: 0.857074\n",
            "Iteration 25000, \t Training Loss: 0.881034, \t Training Accuracy: 0.857144\n",
            "Iteration 26000, \t Training Loss: 0.882580, \t Training Accuracy: 0.857060\n",
            "Iteration 27000, \t Training Loss: 0.884154, \t Training Accuracy: 0.857035\n",
            "Iteration 28000, \t Training Loss: 0.881624, \t Training Accuracy: 0.857059\n",
            "Iteration 29000, \t Training Loss: 0.883509, \t Training Accuracy: 0.857055\n",
            "Iteration 30000, \t Training Loss: 0.880884, \t Training Accuracy: 0.857121\n",
            "Iteration 31000, \t Training Loss: 0.882254, \t Training Accuracy: 0.857112\n",
            "Iteration 32000, \t Training Loss: 0.885332, \t Training Accuracy: 0.857075\n",
            "Iteration 33000, \t Training Loss: 0.880704, \t Training Accuracy: 0.857134\n",
            "Iteration 34000, \t Training Loss: 0.880583, \t Training Accuracy: 0.857232\n",
            "Iteration 35000, \t Training Loss: 0.883210, \t Training Accuracy: 0.857213\n",
            "Iteration 36000, \t Training Loss: 0.882006, \t Training Accuracy: 0.857193\n",
            "Iteration 37000, \t Training Loss: 0.882440, \t Training Accuracy: 0.857171\n",
            "Iteration 38000, \t Training Loss: 0.883173, \t Training Accuracy: 0.857106\n",
            "Iteration 39000, \t Training Loss: 0.881221, \t Training Accuracy: 0.857109\n",
            "Iteration 40000, \t Training Loss: 0.881793, \t Training Accuracy: 0.857072\n",
            "Iteration 41000, \t Training Loss: 0.886748, \t Training Accuracy: 0.857018\n",
            "Final Epoch 10, Training Loss: 0.8867, Training Accuracy: 3.0470\n",
            "Iteration 1000, \t Training Loss: 0.878710, \t Training Accuracy: 0.861094\n",
            "Iteration 2000, \t Training Loss: 0.881338, \t Training Accuracy: 0.859578\n",
            "Iteration 3000, \t Training Loss: 0.884783, \t Training Accuracy: 0.858052\n",
            "Iteration 4000, \t Training Loss: 0.880321, \t Training Accuracy: 0.858070\n",
            "Iteration 5000, \t Training Loss: 0.880832, \t Training Accuracy: 0.857825\n",
            "Iteration 6000, \t Training Loss: 0.880625, \t Training Accuracy: 0.857664\n",
            "Iteration 7000, \t Training Loss: 0.883599, \t Training Accuracy: 0.857315\n",
            "Iteration 8000, \t Training Loss: 0.879915, \t Training Accuracy: 0.857617\n",
            "Iteration 9000, \t Training Loss: 0.883716, \t Training Accuracy: 0.857575\n",
            "Iteration 10000, \t Training Loss: 0.881977, \t Training Accuracy: 0.857514\n",
            "Iteration 11000, \t Training Loss: 0.879151, \t Training Accuracy: 0.857834\n",
            "Iteration 12000, \t Training Loss: 0.880663, \t Training Accuracy: 0.857793\n",
            "Iteration 13000, \t Training Loss: 0.883406, \t Training Accuracy: 0.857517\n",
            "Iteration 14000, \t Training Loss: 0.881936, \t Training Accuracy: 0.857536\n",
            "Iteration 15000, \t Training Loss: 0.883649, \t Training Accuracy: 0.857345\n",
            "Iteration 16000, \t Training Loss: 0.880543, \t Training Accuracy: 0.857405\n",
            "Iteration 17000, \t Training Loss: 0.880585, \t Training Accuracy: 0.857503\n",
            "Iteration 18000, \t Training Loss: 0.884099, \t Training Accuracy: 0.857389\n",
            "Iteration 19000, \t Training Loss: 0.883196, \t Training Accuracy: 0.857283\n",
            "Iteration 20000, \t Training Loss: 0.881610, \t Training Accuracy: 0.857251\n",
            "Iteration 21000, \t Training Loss: 0.884252, \t Training Accuracy: 0.857122\n",
            "Iteration 22000, \t Training Loss: 0.879935, \t Training Accuracy: 0.857171\n",
            "Iteration 23000, \t Training Loss: 0.882456, \t Training Accuracy: 0.857191\n",
            "Iteration 24000, \t Training Loss: 0.883563, \t Training Accuracy: 0.857206\n",
            "Iteration 25000, \t Training Loss: 0.883806, \t Training Accuracy: 0.857034\n",
            "Iteration 26000, \t Training Loss: 0.881534, \t Training Accuracy: 0.857065\n",
            "Iteration 27000, \t Training Loss: 0.882171, \t Training Accuracy: 0.857057\n",
            "Iteration 28000, \t Training Loss: 0.883265, \t Training Accuracy: 0.857021\n",
            "Iteration 29000, \t Training Loss: 0.883897, \t Training Accuracy: 0.856923\n",
            "Iteration 30000, \t Training Loss: 0.881588, \t Training Accuracy: 0.856959\n",
            "Iteration 31000, \t Training Loss: 0.884477, \t Training Accuracy: 0.856910\n",
            "Iteration 32000, \t Training Loss: 0.884472, \t Training Accuracy: 0.856836\n",
            "Iteration 33000, \t Training Loss: 0.885885, \t Training Accuracy: 0.856759\n",
            "Iteration 34000, \t Training Loss: 0.882516, \t Training Accuracy: 0.856791\n",
            "Iteration 35000, \t Training Loss: 0.884262, \t Training Accuracy: 0.856743\n",
            "Iteration 36000, \t Training Loss: 0.883169, \t Training Accuracy: 0.856775\n",
            "Iteration 37000, \t Training Loss: 0.881110, \t Training Accuracy: 0.856819\n",
            "Iteration 38000, \t Training Loss: 0.882428, \t Training Accuracy: 0.856831\n",
            "Iteration 39000, \t Training Loss: 0.881094, \t Training Accuracy: 0.856872\n",
            "Iteration 40000, \t Training Loss: 0.881212, \t Training Accuracy: 0.856893\n",
            "Iteration 41000, \t Training Loss: 0.880854, \t Training Accuracy: 0.856914\n",
            "Final Epoch 10, Training Loss: 0.8809, Training Accuracy: 3.0468\n",
            "Iteration 1000, \t Training Loss: 0.878041, \t Training Accuracy: 0.861781\n",
            "Iteration 2000, \t Training Loss: 0.882693, \t Training Accuracy: 0.858219\n",
            "Iteration 3000, \t Training Loss: 0.879932, \t Training Accuracy: 0.858599\n",
            "Iteration 4000, \t Training Loss: 0.880162, \t Training Accuracy: 0.858922\n",
            "Iteration 5000, \t Training Loss: 0.879599, \t Training Accuracy: 0.859066\n",
            "Iteration 6000, \t Training Loss: 0.883077, \t Training Accuracy: 0.858656\n",
            "Iteration 7000, \t Training Loss: 0.878893, \t Training Accuracy: 0.858719\n",
            "Iteration 8000, \t Training Loss: 0.882232, \t Training Accuracy: 0.858572\n",
            "Iteration 9000, \t Training Loss: 0.878089, \t Training Accuracy: 0.858703\n",
            "Iteration 10000, \t Training Loss: 0.882138, \t Training Accuracy: 0.858675\n",
            "Iteration 11000, \t Training Loss: 0.878174, \t Training Accuracy: 0.858682\n",
            "Iteration 12000, \t Training Loss: 0.881446, \t Training Accuracy: 0.858546\n",
            "Iteration 13000, \t Training Loss: 0.883933, \t Training Accuracy: 0.858209\n",
            "Iteration 14000, \t Training Loss: 0.883972, \t Training Accuracy: 0.857991\n",
            "Iteration 15000, \t Training Loss: 0.877153, \t Training Accuracy: 0.858127\n",
            "Iteration 16000, \t Training Loss: 0.884403, \t Training Accuracy: 0.857905\n",
            "Iteration 17000, \t Training Loss: 0.881999, \t Training Accuracy: 0.857921\n",
            "Iteration 18000, \t Training Loss: 0.881114, \t Training Accuracy: 0.857800\n",
            "Iteration 19000, \t Training Loss: 0.883277, \t Training Accuracy: 0.857656\n",
            "Iteration 20000, \t Training Loss: 0.884671, \t Training Accuracy: 0.857441\n",
            "Iteration 21000, \t Training Loss: 0.882268, \t Training Accuracy: 0.857353\n",
            "Iteration 22000, \t Training Loss: 0.881736, \t Training Accuracy: 0.857373\n",
            "Iteration 23000, \t Training Loss: 0.881397, \t Training Accuracy: 0.857365\n",
            "Iteration 24000, \t Training Loss: 0.883899, \t Training Accuracy: 0.857277\n",
            "Iteration 25000, \t Training Loss: 0.883289, \t Training Accuracy: 0.857252\n",
            "Iteration 26000, \t Training Loss: 0.881297, \t Training Accuracy: 0.857261\n",
            "Iteration 27000, \t Training Loss: 0.883548, \t Training Accuracy: 0.857267\n",
            "Iteration 28000, \t Training Loss: 0.882821, \t Training Accuracy: 0.857239\n",
            "Iteration 29000, \t Training Loss: 0.882261, \t Training Accuracy: 0.857186\n",
            "Iteration 30000, \t Training Loss: 0.882768, \t Training Accuracy: 0.857133\n",
            "Iteration 31000, \t Training Loss: 0.880444, \t Training Accuracy: 0.857116\n",
            "Iteration 32000, \t Training Loss: 0.879373, \t Training Accuracy: 0.857139\n",
            "Iteration 33000, \t Training Loss: 0.881832, \t Training Accuracy: 0.857186\n",
            "Iteration 34000, \t Training Loss: 0.882106, \t Training Accuracy: 0.857162\n",
            "Iteration 35000, \t Training Loss: 0.880488, \t Training Accuracy: 0.857235\n",
            "Iteration 36000, \t Training Loss: 0.881677, \t Training Accuracy: 0.857246\n",
            "Iteration 37000, \t Training Loss: 0.883072, \t Training Accuracy: 0.857161\n",
            "Iteration 38000, \t Training Loss: 0.881638, \t Training Accuracy: 0.857118\n",
            "Iteration 39000, \t Training Loss: 0.883422, \t Training Accuracy: 0.857066\n",
            "Iteration 40000, \t Training Loss: 0.882958, \t Training Accuracy: 0.857027\n",
            "Iteration 41000, \t Training Loss: 0.879390, \t Training Accuracy: 0.857092\n",
            "Final Epoch 10, Training Loss: 0.8794, Training Accuracy: 3.0473\n",
            "Iteration 1000, \t Training Loss: 0.880138, \t Training Accuracy: 0.859375\n",
            "Iteration 2000, \t Training Loss: 0.876992, \t Training Accuracy: 0.860437\n",
            "Iteration 3000, \t Training Loss: 0.880668, \t Training Accuracy: 0.859531\n",
            "Iteration 4000, \t Training Loss: 0.879686, \t Training Accuracy: 0.859473\n",
            "Iteration 5000, \t Training Loss: 0.881402, \t Training Accuracy: 0.859022\n",
            "Iteration 6000, \t Training Loss: 0.881268, \t Training Accuracy: 0.858596\n",
            "Iteration 7000, \t Training Loss: 0.881601, \t Training Accuracy: 0.858507\n",
            "Iteration 8000, \t Training Loss: 0.882194, \t Training Accuracy: 0.858230\n",
            "Iteration 9000, \t Training Loss: 0.879605, \t Training Accuracy: 0.858229\n",
            "Iteration 10000, \t Training Loss: 0.882126, \t Training Accuracy: 0.858034\n",
            "Iteration 11000, \t Training Loss: 0.882244, \t Training Accuracy: 0.857635\n",
            "Iteration 12000, \t Training Loss: 0.878907, \t Training Accuracy: 0.857661\n",
            "Iteration 13000, \t Training Loss: 0.881811, \t Training Accuracy: 0.857427\n",
            "Iteration 14000, \t Training Loss: 0.879942, \t Training Accuracy: 0.857468\n",
            "Iteration 15000, \t Training Loss: 0.877622, \t Training Accuracy: 0.857568\n",
            "Iteration 16000, \t Training Loss: 0.881228, \t Training Accuracy: 0.857523\n",
            "Iteration 17000, \t Training Loss: 0.883049, \t Training Accuracy: 0.857481\n",
            "Iteration 18000, \t Training Loss: 0.880035, \t Training Accuracy: 0.857578\n",
            "Iteration 19000, \t Training Loss: 0.882764, \t Training Accuracy: 0.857526\n",
            "Iteration 20000, \t Training Loss: 0.885294, \t Training Accuracy: 0.857359\n",
            "Iteration 21000, \t Training Loss: 0.881997, \t Training Accuracy: 0.857402\n",
            "Iteration 22000, \t Training Loss: 0.879292, \t Training Accuracy: 0.857513\n",
            "Iteration 23000, \t Training Loss: 0.879186, \t Training Accuracy: 0.857633\n",
            "Iteration 24000, \t Training Loss: 0.882541, \t Training Accuracy: 0.857576\n",
            "Iteration 25000, \t Training Loss: 0.882103, \t Training Accuracy: 0.857566\n",
            "Iteration 26000, \t Training Loss: 0.881230, \t Training Accuracy: 0.857487\n",
            "Iteration 27000, \t Training Loss: 0.881689, \t Training Accuracy: 0.857600\n",
            "Iteration 28000, \t Training Loss: 0.881294, \t Training Accuracy: 0.857610\n",
            "Iteration 29000, \t Training Loss: 0.878285, \t Training Accuracy: 0.857630\n",
            "Iteration 30000, \t Training Loss: 0.877971, \t Training Accuracy: 0.857728\n",
            "Iteration 31000, \t Training Loss: 0.881543, \t Training Accuracy: 0.857716\n",
            "Iteration 32000, \t Training Loss: 0.879845, \t Training Accuracy: 0.857772\n",
            "Iteration 33000, \t Training Loss: 0.881925, \t Training Accuracy: 0.857804\n",
            "Iteration 34000, \t Training Loss: 0.880702, \t Training Accuracy: 0.857817\n",
            "Iteration 35000, \t Training Loss: 0.884676, \t Training Accuracy: 0.857752\n",
            "Iteration 36000, \t Training Loss: 0.882028, \t Training Accuracy: 0.857770\n",
            "Iteration 37000, \t Training Loss: 0.883701, \t Training Accuracy: 0.857677\n",
            "Iteration 38000, \t Training Loss: 0.885435, \t Training Accuracy: 0.857594\n",
            "Iteration 39000, \t Training Loss: 0.881779, \t Training Accuracy: 0.857576\n",
            "Iteration 40000, \t Training Loss: 0.880117, \t Training Accuracy: 0.857600\n",
            "Iteration 41000, \t Training Loss: 0.879640, \t Training Accuracy: 0.857651\n",
            "Final Epoch 10, Training Loss: 0.8796, Training Accuracy: 3.0493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = torch.sigmoid(model(inputs)).squeeze(1)\n",
        "        valid_loss += criterion(outputs, labels).item()\n",
        "\n",
        "        # Compute validation accuracy\n",
        "        predictions = outputs > 0.5\n",
        "        correct_valid += torch.sum(predictions == labels.byte()).item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(train_loader)\n",
        "    accuracy_valid = correct_valid / len(train_dataset)\n",
        "\n",
        "    print(f'\\n VALIDATION: Iteration {i}, '\n",
        "          f'Validation Loss: {avg_valid_loss:.6f}, \\t Validation Accuracy: {accuracy_valid:.6f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA4CrywIabun",
        "outputId": "410e52c3-4ce9-4282-f0b8-707ccb7c8c85"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " VALIDATION: Iteration 41790, Validation Loss: 0.881030, \t Validation Accuracy: 0.863608\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXq5VWjrnOi8",
        "outputId": "6eb65fb8-04e8-41ca-f00f-c1d598e94340"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sp_model = spm.SentencePieceProcessor()\n",
        "sp_model.Load(\"/content/sp_model.model\")\n",
        "word_vec_model = Word2Vec.load(\"/content/word2vec_model\")"
      ],
      "metadata": {
        "id": "zM-3Kf_kabsI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_embedding(answer, sp_model, word_vec_model):\n",
        "  test_sub_words = sp_model.EncodeAsPieces(answer)\n",
        "  # just a part of preprocessing step\n",
        "  if len(test_sub_words[0]) > 1:\n",
        "    test_sub_words[0] = test_sub_words[0][1:]\n",
        "  elif len(test_sub_words[0]) == 1:\n",
        "    test_sub_words = test_sub_words[1:]\n",
        "\n",
        "  embeddings = [word_vec_model.wv[word] for word in test_sub_words if word in word_vec_model.wv]\n",
        "  if embeddings:\n",
        "    final_word_vector = np.mean(embeddings, axis=0)\n",
        "  else:\n",
        "    final_word_vector = np.zeros(100)\n",
        "\n",
        "  return final_word_vector"
      ],
      "metadata": {
        "id": "c8k1tqJtn1d9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_words = [\"pew\", 'adios', 'twist', 'ere', 'mob', 'onset', 'thee', 'tree', 'otis', 'dst', 'quackremedy']\n",
        "\n",
        "for word in testing_words:\n",
        "  input_feature = torch.Tensor(generate_answer_embedding(word, sp_model, word_vec_model))\n",
        "\n",
        "  model.eval().to(\"cpu\")\n",
        "  with torch.no_grad():\n",
        "    output = model(input_feature)\n",
        "    output_probabilities = torch.sigmoid(output)\n",
        "    predictions = (output_probabilities > 0.5).float()\n",
        "    print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeYqlNfUn1a7",
        "outputId": "54fcb096-6c1e-4275-ec6d-624b128f1559"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.])\n",
            "tensor([0.])\n",
            "tensor([1.])\n",
            "tensor([0.])\n",
            "tensor([1.])\n",
            "tensor([1.])\n",
            "tensor([0.])\n",
            "tensor([1.])\n",
            "tensor([0.])\n",
            "tensor([0.])\n",
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion to this approach:\n",
        "1. Obviously it didn't work\n",
        "2. The main flaw with this approach could be the use of sub-word tokenization, and then using word2vec to find the vectors for each of the sub-word, then using mean value as the input to the neural network"
      ],
      "metadata": {
        "id": "ffqeAtSkkVzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/answer_ranker.model')"
      ],
      "metadata": {
        "id": "m8pFFibXqDHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main train_loop\n",
        "num_epochs = 2\n",
        "eval_interval = 500\n",
        "\n",
        "train_eval_interval = 100\n",
        "valid_eval_interval = 500\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_train = 0\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze(dim = 1)\n",
        "        outputs = torch.sigmoid(model(inputs)).squeeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute training accuracy every eval_interval iterations\n",
        "        if i % eval_interval == 0:\n",
        "            predictions = torch.sigmoid(outputs) > 0.5\n",
        "            correct_train += torch.sum(predictions == labels.byte()).item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_loss = 0.0\n",
        "        correct_valid = 0\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs).squeeze(dim = 1)\n",
        "            outputs = torch.sigmoid(model(inputs)).squeeze(1)\n",
        "            valid_loss += criterion(outputs, labels.squeeze(dim = 1)).item()\n",
        "\n",
        "            # Compute validation accuracy\n",
        "            predictions = torch.sigmoid(outputs) > 0.5\n",
        "            correct_valid += torch.sum(predictions == labels.byte()).item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    accuracy_train = correct_train / len(train_dataset)\n",
        "    accuracy_valid = correct_valid / len(valid_dataset)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "          f'Training Loss: {avg_train_loss:.4f}, Training Accuracy: {accuracy_train:.4f}, '\n",
        "          f'Validation Loss: {avg_valid_loss:.4f}, Validation Accuracy: {accuracy_valid:.4f}')"
      ],
      "metadata": {
        "id": "nuN5ZOrgaB8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}